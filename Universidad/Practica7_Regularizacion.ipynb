{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizacion\n",
    "### Propósito: evitar modelos sobreajustados modificando el comportamiento de descenso por gradiente, objetivo y datos\n",
    "### Técnica básica: evaluar la bondad de cualquier modificación mediante estimación del rendimiento teórico (en validación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vamos a ver las opciones para:  Descenso por Gradiente\n",
    "\n",
    "#### Idea intuitiva: queremos alcanzar mínimos profundos en regiones anchas, sin caer en mínimos estrechos\n",
    "\n",
    "##### 1-Terminación temprana: técnica sencilla muy conveniente computacionalmente\n",
    "##### 2-Learning rate constante: si es algo grande, resultará más difícil caer en mínimos estrechos\n",
    "##### 3-Planificador del learning rate: quizás con uno o más ciclos de aumento-decremento para evitar mínimos estrechos\n",
    "##### 4-ReduceLROnPlateau: planificador estándar; caída escalonada monitorizada en validación\n",
    "##### 5-Dropout: técnica muy efectiva y popular que evita el sobreentrenamiento de neuronas individuales (https://keras.io/api/layers/regularization_layers/dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Vamos a ver las opciones para: Objetivo\n",
    "\n",
    "#### Batch size: un batch size algo pequeño añade estocasticidad extra al objetivo y dificulta el sobreajuste\n",
    "#### Penalización de pesos: técnica estándar para penalizar pesos demasiado grandes (en capas seleccionadas)\n",
    "#### Clase Regularizer: https://keras.io/api/layers/regularizers\n",
    "#### Tipos de penalizacion: L1, L2 o L1L2 (Elastic net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Vamos a ver las opciones para: Datos\n",
    "\n",
    "#### Aumento de datos: el aumento de datos dificulta el sobreajuste (de modelos grandes)\n",
    "##### Datos sintéticos: en general se obtienen buenos resultados perturbando adecuadamente los de entrenamiento\n",
    "##### Capas de preproceso de imágenes: https://keras.io/api/layers/preprocessing_layers\n",
    "##### Capas de aumento de imágenes: https://keras.io/api/layers/preprocessing_layers/image_augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio con Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización de los datos sin normalizar (regular) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras; import keras_tuner\n",
    "keras.utils.set_random_seed(23); input_dim = (28, 28, 1); num_classes = 10\n",
    "(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train_val = x_train_val.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train_val = np.expand_dims(x_train_val, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(x_train_val.shape, y_train_val.shape, x_test.shape, y_test.shape)\n",
    "y_train_val = keras.utils.to_categorical(y_train_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train_val[:-10000]; x_val = x_train_val[-10000:]\n",
    "y_train = y_train_val[:-10000]; y_val = y_train_val[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MyHyperModel: exploramos aumento de datos (rotación, translación y zoom) y dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        M = keras.Sequential()\n",
    "        M.add(keras.Input(shape=(28, 28, 1)))\n",
    "        factor = hp.Float(\"factor\", min_value=0.01, max_value=0.3, step=2, sampling=\"log\")\n",
    "        M.add(keras.layers.RandomRotation(factor, fill_mode=\"nearest\"))\n",
    "        M.add(keras.layers.RandomTranslation(factor, factor, fill_mode=\"nearest\"))\n",
    "        M.add(keras.layers.RandomZoom(factor, fill_mode=\"nearest\"))\n",
    "        M.add(keras.layers.Rescaling(1./255))\n",
    "        filters = 64\n",
    "        M.add(keras.layers.Conv2D(filters, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        M.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        M.add(keras.layers.Conv2D(2*filters, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        M.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        M.add(keras.layers.Flatten())\n",
    "        M.add(keras.layers.Dense(units=800, activation='relu'))\n",
    "        # dropout = hp.Float(\"dropout\", min_value=0.0, max_value=0.5, step=0.1)\n",
    "        dropout = 0.5\n",
    "        M.add(keras.layers.Dropout(dropout))\n",
    "        M.add(keras.layers.Dense(10, activation='softmax'))\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.00168)\n",
    "        M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "        return M\n",
    "    def fit(self, hp, M, x, y, xy_val, **kwargs):\n",
    "        factor = 0.3787; patience = 5\n",
    "        reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy', factor=factor, patience=patience, min_delta=1e-4, min_lr=1e-5)\n",
    "        early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2*patience, min_delta=1e-5)\n",
    "        kwargs['callbacks'].extend([reduce_cb, early_cb])\n",
    "        return M.fit(x, y, batch_size=256, epochs=20, validation_data=xy_val, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento: exploración y resumen de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "MyHyperModel(), objective=\"val_accuracy\", max_trials=10, executions_per_trial=1,\n",
    "overwrite=True, directory=\"tmp\", project_name=\"FASHION-MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 12s]\n",
      "val_accuracy: 0.9164000153541565\n",
      "\n",
      "Best val_accuracy So Far: 0.9189000129699707\n",
      "Total elapsed time: 00h 11m 58s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Se sacan los mejores 3 resultados dados en el experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in /tmp/FASHION-MNIST\n",
      "Showing 3 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "factor: 0.01\n",
      "Score: 0.9189000129699707\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "factor: 0.01\n",
      "Score: 0.9186000227928162\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "factor: 0.01\n",
      "Score: 0.9174000024795532\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimento (cont.): evaluación en test de los mejores modelos en validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/tf217/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: Hyperparameters: {'factor': 0.01} Loss: 0.2918 Precisión: 91.88%\n",
      "Model 1: Hyperparameters: {'factor': 0.01} Loss: 0.2876 Precisión: 91.46%\n",
      "Model 2: Hyperparameters: {'factor': 0.01} Loss: 0.3175 Precisión: 91.28%\n",
      "Model 3: Hyperparameters: {'factor': 0.01} Loss: 0.2916 Precisión: 91.66%\n",
      "Model 4: Hyperparameters: {'factor': 0.01} Loss: 0.3277 Precisión: 91.25%\n",
      "Model 5: Hyperparameters: {'factor': 0.01} Loss: 0.2894 Precisión: 90.93%\n",
      "Model 6: Hyperparameters: {'factor': 0.02} Loss: 0.2749 Precisión: 91.11%\n",
      "Model 7: Hyperparameters: {'factor': 0.04} Loss: 0.284 Precisión: 90.63%\n",
      "Model 8: Hyperparameters: {'factor': 0.08} Loss: 0.3048 Precisión: 88.91%\n",
      "Model 9: Hyperparameters: {'factor': 0.16} Loss: 0.4456 Precisión: 83.69%\n"
     ]
    }
   ],
   "source": [
    "num_models = 10\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=num_models)\n",
    "best_models = tuner.get_best_models(num_models=num_models)\n",
    "for m in range(num_models):\n",
    "    values = best_hyperparameters[m].values\n",
    "    score = best_models[m].evaluate(x_test, y_test, verbose=0)\n",
    "    print(f'Model {m}: Hyperparameters: {values!s} Loss: {score[0]:.4} Precisión: {score[1]:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf217",
   "language": "python",
   "name": "tf217"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
